{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection - Autocorrelation - Pearson Correlation Coefficients Filter Method\n",
    "\n",
    "A dataset can contain correlated features. Two or more than two features are correlated if they are close to each other in the linear space.\n",
    "\n",
    "Goal is to remove the corellated features, i.e., features which are simillar to other features.\n",
    "\n",
    "We will reomve the colinearilty using the Pearson Correlation Coefficient.\n",
    "\n",
    "### Summary:\n",
    " - Feature Space to target correlation is desired\n",
    " - Feature to feature correlation is not desired\n",
    " - If two or more features are highly correlated thn eiter feature is redundant\n",
    " - Correlation in feature space increases model complexity\n",
    " - Removing correlated features improves model performance\n",
    " - Different model shows different performance over the correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# VarianceThreshold - Feature selector that removes all low-variance features.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/santander.csv\", nrows=20000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(\"TARGET\", axis=1)  # Features\n",
    "y = data[\"TARGET\"]  # Outcome\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Features Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "constant_filter.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of features after constants removal\n",
    "constant_filter.get_support().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns True for all the features which are constants.\n",
    "constant_list = [\n",
    "    not temp for temp in constant_filter.get_support()\n",
    "]  # Inversing the True to False and False to True\n",
    "constant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of all the features which are constants\n",
    "x.columns[constant_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all the constants from our Training and Test dataset.\n",
    "x_train_filter = constant_filter.transform(x_train)\n",
    "x_test_filter = constant_filter.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take a look at the original and the transformed data (after removing the constants)\n",
    "x_train.shape, x_test.shape, x_train_filter.shape, x_test_filter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi Constants Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_constant_filter = VarianceThreshold(threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_constant_filter.fit(x_train_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_constant_filter.get_support().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_quasi_filter = quasi_constant_filter.transform(x_train_filter)\n",
    "x_test_quasi_filter = quasi_constant_filter.transform(x_test_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take a look at the original and the transformed data (after removing the constants)\n",
    "x_train.shape, x_test.shape, x_train_filter.shape, x_test_filter.shape, x_train_quasi_filter.shape, x_train_quasi_filter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Features Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_T = x_train_quasi_filter.T\n",
    "x_test_T = x_test_quasi_filter.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see the pandas dataframe has been transformed in to numpy array after transpose.\n",
    "type(x_train_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing numpy array back to pandas dataframe\n",
    "x_train_T = pd.DataFrame(x_train_T)\n",
    "x_test_T = pd.DataFrame(x_test_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see after transpose the rows has become columns and columns has become rows.\n",
    "x_train_T.shape, x_test_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting duplicate features count\n",
    "x_train_T.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_features = x_train_T.duplicated()\n",
    "duplicated_features\n",
    "\n",
    "# True is duplicated and False is non duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duppicated features.\n",
    "# After this the False becomes True and True becomes false.\n",
    "\n",
    "# Inversing the True to False and False to True\n",
    "features_to_keep = [not index for index in duplicated_features]\n",
    "features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset after removing constants, quasi constants and duplicates.\n",
    "\n",
    "# Transposing again to original form\n",
    "x_train_unique = x_train_T[features_to_keep].T\n",
    "\n",
    "# Transposing again to original form\n",
    "x_test_unique = x_test_T[features_to_keep].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, x_train_unique.shape, x_test_unique.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model and Compare the Performance after and before removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest(x_train, x_test, y_train, y_test):\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"Accuracy on test set: \")\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run on final data.\n",
    "run_random_forest(x_train_unique, x_test_unique, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run on original data.\n",
    "run_random_forest(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy and time taken is less after removing the constants, quasi constants and duplicates compare to the original data. \n",
    "\n",
    "What we can say here is that removing constants, quasi constants and duplicates doesn't depricates the accuracy it rather improves it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Correlated Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = x_train_unique.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corrmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data -  Training or Testing data\n",
    "# threshold - Point after which the data will be discarded\n",
    "def get_correlation(data, threshold):\n",
    "\n",
    "    corr_col = set()\n",
    "    corrmat = data.corr()\n",
    "\n",
    "    # Navigating from feature to feature\n",
    "    for i in range(len(corrmat.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corrmat.iloc[i, j]) > threshold:\n",
    "                colname = corrmat.columns[i]\n",
    "                corr_col.add(colname)\n",
    "    return corr_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = get_correlation(x_train_unique, 0.85)\n",
    "corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all correlated features\n",
    "x_train_uncorr = x_train_unique.drop(labels=corr_features, axis=1)\n",
    "x_test_uncorr = x_test_unique.drop(labels=corr_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_uncorr.shape, x_test_uncorr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_random_forest(x_train_uncorr, x_test_uncorr, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the accurracy is still close to the accuracy with original dataset but the total training time has been brought down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case we discarded all the correlated features but now we will try with keeping one of the most important features. The feature which gives more information will be kept and the other will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertically stacking the features. Ex., 0 vs 0 to 244\n",
    "corrdata = corrmat.abs().stack()\n",
    "corrdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting all values in descending order\n",
    "corrdata = corrdata.sort_values(ascending=False)\n",
    "corrdata\n",
    "\n",
    "# Now we can see the correlated data stacked together.\n",
    "# Ex., Feature 29 is correlated with 58 and 58 is correlated with 29 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrdata = corrdata[corrdata > 0.85]\n",
    "corrdata = corrdata[corrdata < 1]\n",
    "corrdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to Pandas Dataframe\n",
    "corrdata = pd.DataFrame(corrdata).reset_index()\n",
    "corrdata.columns = [\"Features1\", \"Features2\", \"Corr_Value\"]\n",
    "corrdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the correlated features. i.e, Group all the features which are correlated to 0 and so on.\n",
    "grouped_features_list = []\n",
    "correlated_groups_list = []\n",
    "\n",
    "for feature in corrdata.Features1.unique():\n",
    "    if feature not in grouped_features_list:\n",
    "        correlated_block = corrdata[corrdata.Features1 == feature]\n",
    "        grouped_features_list = (\n",
    "            grouped_features_list\n",
    "            + list(correlated_block.Features2.unique())\n",
    "            + [feature]\n",
    "        )\n",
    "        correlated_groups_list.append(correlated_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correlated_groups_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in correlated_groups_list:\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance - Based on Tree based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = []\n",
    "for group in correlated_groups_list:\n",
    "\n",
    "    # Total features\n",
    "    features = list(group.Features1.unique()) + list(group.Features2.unique())\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    rf.fit(x_train_unique[features], y_train)\n",
    "\n",
    "    importance = pd.concat(\n",
    "        [pd.Series(features), pd.Series(rf.feature_importances_)], axis=1\n",
    "    )\n",
    "    importance.columns = [\"Features\", \"Importance\"]\n",
    "    importance.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "    most_important_feature = importance.iloc[0]\n",
    "    important_features.append(most_important_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = pd.DataFrame(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_consider = set(important_features[\"Features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_discard = set(corr_features) - set(features_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_discard = list(features_to_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_grouped_uncorr = x_train_unique.drop(labels=features_to_discard, axis=1)\n",
    "x_train_grouped_uncorr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_grouped_uncorr = x_test_unique.drop(labels=features_to_discard, axis=1)\n",
    "x_test_grouped_uncorr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Original Data\n",
    "run_random_forest(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# After discarding all correlated data\n",
    "run_random_forest(x_train_uncorr, x_test_uncorr, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# After keeping most important correlated data\n",
    "run_random_forest(x_train_grouped_uncorr, x_test_grouped_uncorr, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are handling a very large dataset then the feature selection is very important."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
